---
title: Bayesian statistical inference
help-section: Baysig quick tour
help-section-order: 3
---

This is the third document in a quick tour of the Baysig programming language, following on from [[Baysig quick tour: probability distributions]]. The tour begins in [[Baysig quick tour: fundamentals]].

When we move from simple to more complex distributions, the
connection to statistical modelling becomes clearer:

> regress = prob
>    offset  ~  uniform 1.0 5.0
>    err     ~  gamma 1 1
>    slope   ~  uniform (-0.5) 1.0
>    xys     ~  repeat 30  ( prob x ~ uniform 0.0 1.0
>                                 y ~ normal (offset+slope*x) err
>                                 return (x,y) 
>                          )
>    return  xys
 
`regress` can be interpreted in two ways. It is a piece of computer
code that generates some random data, in this case a list of pairs of
floating-point values. It can also be read as a hierarchical
probability distribution, with the individual random number generators
denoting elementary probability distributions. If we view this as a
statistical model for some process observed in nature, `sample`ing from
this distribution generates simulated data. 

Here's some data generated in such a way:

?> scatterPlot theData

An experimental scientist is unlikely to be impressed. He or she is
already in possession of actually observed data and has little use for
"fake" data. However, because `regress` is a *probabilistic* model, we can
use Bayes' Theorem to "invert" it. That is, we can combine the model
and the data and derive the likely values of the unknown parameters
(`offset`, `err` and `slope` above). To put it differently, assuming
for the moment that regress is the computational process that
generated the observed data, what are the most likely value to have
been generated by the individual random number generators that make up
`regress`? Inversion thus turns a model into a data analysis tool. In
Bayesian terminology, the first three lines of `regress` denote the
*prior,* which here is part of the model. 

This operation cannot be expressed as a simple function because
depends on being able to "look inside" regress, so we have defined it
as a top-level action.

>> theData <* sample regress1

>> samples = 2000

> regrParams <* estimate regress theData

`estimate` returns, here in the variable `regrParams`, the probability
distribution Bayesians call the "posterior," i.e. the distribution of
the parameters given the observed data. The type of `regrParams` is a
probability distribution over a record containing as named fields the
parameters in the `regress` model. If we ask Baysig to print this
variable, it displays the mean and the standard deviation of each variable:

?> regrParams

But that is an incomplete summary of the information available in this
variable. This summary does not address important questions such as
the shape of the posterior distribution (e.g. skew, kurtosis or
multiple peaks) or correlations in the estimates of individual
fields. But such information is still present in the variable
`regrParams`. For instance, we can plot the marginal distribution of a
single parameter:

?> distPlot (with regrParams offset)

or a scatter plot of samples from the estimates of two parameters

>> scatter <* sample (repeat 500 $ with regrParams (slope,offset))

?> scatterPlot scatter

Therefore, non-Gaussian distribution shape and correlation between
parameters are retained in the output of `estimate`. Perhaps more
fundamentally, `regrParams` is a probability distribution and can be
sampled from in order to form further probability
distributions. `with`, as defined above, is a thin layer of
syntactical sugar for doing this.

Classical statistics invites us to consider whether model parameters
are "significant" by evaluating the probability that certain
statistics (calculations from the data) exceed the observed statistic
in a model (the "null model" or "null hypothesis") in which said model
parameter equals exactly zero. That sounds convoluted -- it is
convoluted and almost impossible to interpret for complex models --
and it tells us nothing about the probability that the parameter
really is zero. In the Bayesian approach, we directly ask the
question: could this parameter possibly be zero? So we draw out the
distribution of the parameter given the data. For instance, we might
be interested in whether the slope in the regress model could possibly
be zero (no correlation):

?> distPlot (with regrParams slope)

In this graphical plot it is clear that the distribution of the slope
given the data *does not* include zero. It is therefore highly
improbably that the slope is zero; or, we can "reject" that possibility.

If the outcome of such a graphical check is not clear-cut, we can also
directly evaluate the probability of a hypothesis. In Baysig, a
hypothesis is simply represented as a probability distribution over
the Booleans (True or False). We might be tempted to evaluate the
probability that `slope` equals exactly zero. Unfortunately, this
probability is *always* zero, no matter what the data looks like (an
elementary result of probability theory is that for *any* continuously
distributed random variable, the probability that it takes any
particular value equals zero). Instead, we can evaluate the probility
that `slope` is greater than zero. That hypothesis can be given a name
and a formal expression:

> slopePositive = with regrParams (slope > 0.0)

and be evaluated:

?> slopePositive

That is the probability of the hypothesis being true given the
data, something that can *never* be evaluated with classical statistics.

Occasionally it is useful to be able to sample from a model with known
parameters or with a distribution of parameters estimated from
data. The former situation is used for testing the precision of
estimation; the latter for for doing "posterior predictive checks"
which assess whether the model is consistent with the data and can be
seen as Bayesian equivalents of significance tests. The `update`
operation changes a model such that parameters are drawn from a
specified probability distribution. To sample from `regress` with
known parameters:

> regress1 <* updateS regress (return {err=>0.01;
>                                      offset=>2.0;
>                                      slope=>0.4})

or with the posterior

< regress2 <* update regress regrParams

`sample`, `estimate` and `update` together with probabilistic models
written in the random number monad forms a programming interface to
fully Bayesian computing. In this language, we program directly with
probability distributions over arbitrary data types. The next
section illustrates how this extends to models of time-series data.

The quick tour of Baysig continues in the document [[Baysig quick tour: dynamical systems]].
                                          
